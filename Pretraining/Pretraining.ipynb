{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0302dd12-2c26-469f-9004-9e7e3b7205b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 15:49:37.982862: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import sklearn\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "import collections\n",
    "import wandb\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoConfig, \n",
    "                          AutoModelForMaskedLM,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          Trainer, \n",
    "                          TrainingArguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def6461-659a-425e-9d89-0a63e057d3e9",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52705d3a-1f2f-49c5-a391-43c32940721e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...</td>\n",
       "      <td>{'word': '비틀즈', 'start_idx': 24, 'end_idx': 26...</td>\n",
       "      <td>{'word': '조지 해리슨', 'start_idx': 13, 'end_idx':...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...</td>\n",
       "      <td>{'word': '민주평화당', 'start_idx': 19, 'end_idx': ...</td>\n",
       "      <td>{'word': '대안신당', 'start_idx': 14, 'end_idx': 1...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...</td>\n",
       "      <td>{'word': '광주FC', 'start_idx': 21, 'end_idx': 2...</td>\n",
       "      <td>{'word': '한국프로축구연맹', 'start_idx': 34, 'end_idx...</td>\n",
       "      <td>org:member_of</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...</td>\n",
       "      <td>{'word': '아성다이소', 'start_idx': 13, 'end_idx': ...</td>\n",
       "      <td>{'word': '박정부', 'start_idx': 22, 'end_idx': 24...</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...</td>\n",
       "      <td>{'word': '요미우리 자이언츠', 'start_idx': 22, 'end_id...</td>\n",
       "      <td>{'word': '1967', 'start_idx': 0, 'end_idx': 3,...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence  \\\n",
       "0   0  〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...   \n",
       "1   1  호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...   \n",
       "2   2  K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...   \n",
       "3   3  균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...   \n",
       "4   4  1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...   \n",
       "\n",
       "                                      subject_entity  \\\n",
       "0  {'word': '비틀즈', 'start_idx': 24, 'end_idx': 26...   \n",
       "1  {'word': '민주평화당', 'start_idx': 19, 'end_idx': ...   \n",
       "2  {'word': '광주FC', 'start_idx': 21, 'end_idx': 2...   \n",
       "3  {'word': '아성다이소', 'start_idx': 13, 'end_idx': ...   \n",
       "4  {'word': '요미우리 자이언츠', 'start_idx': 22, 'end_id...   \n",
       "\n",
       "                                       object_entity  \\\n",
       "0  {'word': '조지 해리슨', 'start_idx': 13, 'end_idx':...   \n",
       "1  {'word': '대안신당', 'start_idx': 14, 'end_idx': 1...   \n",
       "2  {'word': '한국프로축구연맹', 'start_idx': 34, 'end_idx...   \n",
       "3  {'word': '박정부', 'start_idx': 22, 'end_idx': 24...   \n",
       "4  {'word': '1967', 'start_idx': 0, 'end_idx': 3,...   \n",
       "\n",
       "                       label     source  \n",
       "0                no_relation  wikipedia  \n",
       "1                no_relation   wikitree  \n",
       "2              org:member_of   wikitree  \n",
       "3  org:top_members/employees   wikitree  \n",
       "4                no_relation  wikipedia  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('/opt/ml/dataset/train/train.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "540948cb-8fa9-458b-965c-aa4bfc5e2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(data_df)\n",
    "data_sen = list(data_df['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f2d278-acf4-4a71-827a-0a5c759f871c",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9094aeb0-bd36-430d-a91b-9cf53bd235a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64127715-3c74-4a3c-a98f-b043941d09b0",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b69a969-9b82-46d7-9fd8-65d00b245de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'klue/roberta-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de93161-a1da-4312-9684-f167ad46df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config =  AutoConfig.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name, config=model_config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8340c803-0afa-4c77-89b1-8697703626fd",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "925d2602-6ce7-4847-81fb-d452f15be5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, sen_data, len_data, tokenizer, val_ratio=0.1):\n",
    "        super(PretrainDataset, self).__init__()\n",
    "        assert len(sen_data) == len(len_data)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.val_ratio = 0.1\n",
    "\n",
    "        self.dataset = []\n",
    "        for i in tqdm(range(5)) :\n",
    "            tensor_data = self.build_data(sen_data, len_data)\n",
    "            self.dataset.extend(tensor_data)\n",
    "\n",
    "    def pair_data(self, sen_data, len_data) :\n",
    "        data_size = len(sen_data)\n",
    "        len_group = collections.defaultdict(list)\n",
    "\n",
    "        for i, length in enumerate(len_data) :\n",
    "            len_value = length // 10\n",
    "            len_group[len_value].append(i)\n",
    "\n",
    "        data_sen_idx = []\n",
    "        for group in sorted(len_group.keys()) :\n",
    "            idx_list = len_group[group]\n",
    "            random.shuffle(idx_list)\n",
    "            data_sen_idx.extend(idx_list)\n",
    "\n",
    "        fir_data = []\n",
    "        sec_data = []\n",
    "\n",
    "        for i in range(int(data_size/2)) :\n",
    "            fir_idx = data_sen_idx[i]\n",
    "            sec_idx = data_sen_idx[data_size-1-i]\n",
    "\n",
    "            fir_sen = sen_data[fir_idx]\n",
    "            sec_sen = sen_data[sec_idx]\n",
    "\n",
    "            if i % 2 == 0 :\n",
    "                fir_data.append(fir_sen)\n",
    "                sec_data.append(sec_sen)\n",
    "            else :\n",
    "                fir_data.append(sec_sen)\n",
    "                sec_data.append(fir_sen)\n",
    "    \n",
    "        return fir_data, sec_data\n",
    "\n",
    "\n",
    "    def build_data(self, sen_data, len_data) :\n",
    "        data_size = len(sen_data)\n",
    "        fir_data, sec_data = self.pair_data(sen_data, len_data)\n",
    "\n",
    "        tensor_data = []\n",
    "        for i in range(int(data_size/2)) :\n",
    "            data_dict = self.tokenizer(fir_data[i],\n",
    "                                       sec_data[i],\n",
    "                                       return_tensors='pt',\n",
    "                                       return_token_type_ids=False,\n",
    "                                       add_special_tokens=True)\n",
    "      \n",
    "            data_dict = {k : v[0] for k, v in data_dict.items()}\n",
    "            tensor_data.append(data_dict)\n",
    "        return tensor_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def split(self) :\n",
    "        n_val = int(len(self) * self.val_ratio)\n",
    "        n_train = len(self) - n_val\n",
    "        train_set, val_set = random_split(self, [n_train, n_val])\n",
    "        \n",
    "        return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a145cec-73d3-419d-89d6-432178704894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32470/32470 [00:08<00:00, 3951.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Length of Data\n",
    "data_len = [len(tokenizer.tokenize(sen)) for sen in tqdm(data_sen)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4699903a-595b-4e8a-9d9d-84e306017da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:41<00:00,  8.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "dset = PretrainDataset(data_sen, data_len, tokenizer)\n",
    "train_dset, val_dset = dset.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e865431e-3733-4b89-bd4b-ddc87098282d",
   "metadata": {},
   "source": [
    "## Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24f46c4f-aa55-4535-813c-3b85d5644b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
    "                                                mlm=True, \n",
    "                                                mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc3957-4f4f-43b6-b4ab-2a21a762cbef",
   "metadata": {},
   "source": [
    "## Training Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "799eeeaf-144e-47e6-b1e7-f58942f6df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Argument\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    save_total_limit=5,\n",
    "    save_steps=1000, \n",
    "    num_train_epochs=5,\n",
    "    learning_rate=1e-5, \n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=4000,  \n",
    "    weight_decay=1e-2,  \n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 500,\n",
    "    logging_dir='./logs', \n",
    "    logging_steps=500,  \n",
    "    report_to='wandb'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956be7b9-894b-4d38-9b07-ef675098eef8",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa5435d6-088f-48e7-aa04-9166a334a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "  model=model, \n",
    "  args=training_args, \n",
    "  data_collator=data_collator,\n",
    "  train_dataset=train_dset,\n",
    "  eval_dataset=val_dset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c226087-a5e6-4741-9bf1-82b4691fa387",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10c008-55c1-4334-b174-8fea46a5ba3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msangha0411\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "2021-10-03 15:50:50.524851: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">pretraining</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sangha0411/huggingface\" target=\"_blank\">https://wandb.ai/sangha0411/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sangha0411/huggingface/runs/2xtinxks\" target=\"_blank\">https://wandb.ai/sangha0411/huggingface/runs/2xtinxks</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/team/klue-level2-nlp-02/Pretraining/wandb/run-20211003_155049-2xtinxks</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 73058\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 22835\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3880' max='22835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3880/22835 45:51 < 3:44:10, 1.41 it/s, Epoch 0.85/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.293700</td>\n",
       "      <td>1.830008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.844400</td>\n",
       "      <td>1.608319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.681800</td>\n",
       "      <td>1.517828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.586600</td>\n",
       "      <td>1.464435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.571100</td>\n",
       "      <td>1.432953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.544200</td>\n",
       "      <td>1.403903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.522300</td>\n",
       "      <td>1.390225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8117\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8117\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8117\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8117\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8117\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8117\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-3000\n",
      "Configuration saved in ./results/checkpoint-3000/config.json\n",
      "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8117\n",
      "  Batch size = 16\n"
     ]
    }
   ],
   "source": [
    "WANDB_AUTH_KEY = os.getenv('WANDB_AUTH_KEY')\n",
    "wandb.login(key=WANDB_AUTH_KEY)\n",
    "\n",
    "wandb.init(entity=\"sangha0411\",project=\"huggingface\",name=\"pretraining\")\n",
    "trainer.train()\n",
    "wandb.finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8df25a-6372-432e-8714-64cdc12bab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25359637-17fa-44b8-81c0-86a999e1ff1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
