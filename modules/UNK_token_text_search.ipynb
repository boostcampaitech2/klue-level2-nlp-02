{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from typing import  Dict, List, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-10-02 23:08:58.688222: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "Train_csv_file = '/opt/ml/dataset/train/train.csv'\n",
    "Test_csv_file = '/opt/ml/dataset/test/test_data.csv'\n",
    "\n",
    "df = pd.read_csv(Train_csv_file)\n",
    "df = df.drop_duplicates(['sentence','subject_entity','object_entity','label']).reset_index()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "MODEL_NAME = \"klue/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "vocab = list(tokenizer.get_vocab().keys())\n",
    "unused_list = [word for word in vocab if word.startswith('[unused')]        \n",
    "print(f'unused token count: {len(unused_list)}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "unused token count: 500\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 🛰️ 전처리"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def remove_special_char(sentence):\n",
    "    \"\"\" 특수문자 및 독일어 제거, 수정\"\"\"\n",
    "    sentence = re.sub(r'[À-ÿ]+','', sentence) # 독일어\n",
    "    sentence = re.sub(r'[\\u0600-\\u06FF]+','', sentence)  # 사우디어\n",
    "    sentence = re.sub(r'[\\u00C0-\\u02B0]+','', sentence)  # 라틴어\n",
    "    sentence = re.sub(r'[ß↔Ⓐب€☎☏±∞]+','', sentence)\n",
    "    sentence = re.sub('–','─', sentence)\n",
    "    sentence = re.sub('⟪','《', sentence)\n",
    "    sentence = re.sub('⟫','》', sentence)\n",
    "    sentence = re.sub('･','・', sentence)\n",
    "    sentence = re.sub('µ','ℓ', sentence)\n",
    "    sentence = re.sub('®','㈜', sentence)\n",
    "    sentence = re.sub('～','㈜', sentence)\n",
    "    return sentence\n",
    "\n",
    "test_sen = 'Hermann Müller'\n",
    "remove_special_char(test_sen)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Hermann Mller'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def add_space_char(sentence) :\n",
    "    def add_space(match) :\n",
    "        res_str = ', '.join(match.group().split(',')).rstrip()\n",
    "        return res_str\n",
    "    p = re.compile(r'([기-힣\\w\\-]+,)+[기-힣\\w\\-]+')\n",
    "    sentence = p.sub(add_space, sentence)\n",
    "    return sentence\n",
    "test_sen = '앨범에는 에미넴,G-Unit,닥터드레,제이미 폭스 등이 참여하고,영국차트에서도 1위를 한다.'\n",
    "add_space_char(test_sen)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'앨범에는 에미넴, G-Unit, 닥터드레, 제이미 폭스 등이 참여하고, 영국차트에서도 1위를 한다.'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def substitution_date(sentence):\n",
    "    \"\"\"\n",
    "    기간 표시 '-' => '~'\n",
    "    1223년 – => 1223년 ~ \n",
    "    \"\"\"\n",
    "    def sub_tibble(match) :\n",
    "        res_str = re.sub('[–\\-]','~',match.group())\n",
    "        return res_str\n",
    "    re_patterns = [\n",
    "        r'(\\d{2,4}년\\s*)(\\d{1,2}[월|일]\\s*)(\\d{1,2}[월|일])\\s*[–\\-]',\n",
    "        r'(\\d{2,4}년\\s*)(\\d{1,2}[월|일]\\s*)\\s*[–\\-]',\n",
    "        r'(\\d{2,4}년\\s*)\\s*[–\\-]',\n",
    "        r'\\((\\d{4}[–\\-]\\d{2,4})\\)'\n",
    "    ]\n",
    "    for re_pattern in re_patterns :\n",
    "        p = re.compile(re_pattern)\n",
    "        sentence = p.sub(sub_tibble, sentence)   \n",
    "    return sentence\n",
    "\n",
    "test_sen = '후 시니어 대회로 올라가서 (1934–1942) 시즌 ISU 쇼트트랙 월드컵에 4차례 출전하여 금메달 4개, 은메달 5개를 차지하였으며, 2013-14 시즌에서 김아랑은 쇼트트랙 스피드'\n",
    "substitution_date(test_sen)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'후 시니어 대회로 올라가서 (1934~1942) 시즌 ISU 쇼트트랙 월드컵에 4차례 출전하여 금메달 4개, 은메달 5개를 차지하였으며, 2013-14 시즌에서 김아랑은 쇼트트랙 스피드'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def add_space_year(sentence):\n",
    "    \"\"\"\n",
    "    숫자와 년 사이에 공백\n",
    "    1223년 => 1223 년 => ⑦ 년\n",
    "    \"\"\"\n",
    "    def add_space(match) :\n",
    "        # res_str = '⑦ ' + match.group()[4:]\n",
    "        res_str =  match.group()[:4] +' ' + match.group()[4:]\n",
    "        return res_str\n",
    "    p = re.compile(r'\\d{4}년')\n",
    "    sentence = p.sub(add_space, sentence)\n",
    "    return sentence\n",
    "test_sen = '2010년에는 아시아 가수 최초로 마이클 잭슨의 곡을 리메이크하였는데'\n",
    "add_space_year(test_sen)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2010 년에는 아시아 가수 최초로 마이클 잭슨의 곡을 리메이크하였는데'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def preprocessing(sentence) :\n",
    "    sent = remove_special_char(sentence)\n",
    "    sent = substitution_date(sent)\n",
    "    # sent = add_space_year(sent)\n",
    "    sent = add_space_char(sent)\n",
    "    return sent"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "df = pd.read_csv(Train_csv_file)\n",
    "\n",
    "ess = ['sentence','subject_entity','object_entity']\n",
    "preprocessed_df = df.copy()\n",
    "for col in ess :\n",
    "    preprocessed_df[col] = preprocessed_df[col].apply(preprocessing)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 🛰️UNK으로 변하는 word 및 char 확인"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from IPython.core.display import HTML\n",
    "def word_highligt_html(txt, word, color='black', highlight=None, attr=None):\n",
    "    if isinstance(word, str):\n",
    "        txt = txt.replace(word, f'<span style=\"color: {color}; background-color:{highlight}\">{word}</span>')\n",
    "    else:\n",
    "        if not isinstance(color, list):\n",
    "            color = [color] * len(word)\n",
    "        if not isinstance(highlight, list):\n",
    "            highlight = [highlight] * len(word)\n",
    "        for w, c, h in zip(word, color, highlight):\n",
    "            txt = txt.replace(w, f'<span style=\"color: {c}; background-color:{h}\">{w}</span>')\n",
    "    return txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def subword_parsing(wordpiece:List) -> List[str]: ## subword # 제거용\n",
    "    Known_char = []\n",
    "    for subword in wordpiece :\n",
    "        if subword == tokenizer.unk_token :\n",
    "            Known_char.append(tokenizer.unk_token)\n",
    "        else :\n",
    "            string = subword.replace('#','')\n",
    "            Known_char.extend(string)\n",
    "    return Known_char\n",
    "\n",
    "\n",
    "def UNK_word_and_chr(text:str) -> Tuple[List[str], List[str]]:\n",
    "    sub_word_UNK_list = []\n",
    "    \n",
    "    def add_space(match) :\n",
    "        bracket = match.group()\n",
    "        added = ' ' + bracket + ' '\n",
    "        return added\n",
    "    p = re.compile(r'[\\([)\\]|,|-|~|-|‘|’|\"|\\']')\n",
    "    words_list = p.sub(add_space, text).split()\n",
    "    for word in words_list :\n",
    "        subwordpieces_ID_encoded = tokenizer.tokenize(word)\n",
    "        Known_subword = subword_parsing(subwordpieces_ID_encoded)\n",
    "        for sub_char, NK_char in zip(word, Known_subword) :\n",
    "            if sub_char != NK_char and len(word) == len(Known_subword) :\n",
    "                sub_word_UNK_list.append(sub_char)\n",
    "            elif sub_char != NK_char and len(word) != len(Known_subword) :\n",
    "                sub_word_UNK_list.append(word)\n",
    "                break\n",
    "    return sub_word_UNK_list\n",
    "    \n",
    "text ='박용오(朴容旿, 1937년 4월 29일(음력 3월 19일)(음력 3월 19일) ~ 2009년 11월 4일)는 서울에서 태어난 대한민국의 기업인으로 두산그룹 회장, KBO 총재 등을 역임했다.'\n",
    "print(UNK_word_and_chr(text))\n",
    "if tokenizer.unk_token in tokenizer.tokenize(text) :\n",
    "    print(tokenizer.tokenize(text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['容', '旿']\n",
      "['박용', '##오', '(', '朴', '[UNK]', '[UNK]', ',', '1937', '##년', '4', '##월', '29', '##일', '(', '음력', '3', '##월', '19', '##일', ')', '(', '음력', '3', '##월', '19', '##일', ')', '~', '2009', '##년', '11', '##월', '4', '##일', ')', '는', '서울', '##에서', '태어난', '대한민국', '##의', '기업인', '##으로', '두산', '##그룹', '회장', ',', 'KBO', '총재', '등', '##을', '역임', '##했', '##다', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 🛰️ UNK 분포"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "txt = ''\n",
    "count = 1\n",
    "unk_len = 0\n",
    "for sen in tqdm(preprocessed_df['sentence']) :\n",
    "    if tokenizer.unk_token in tokenizer.tokenize(sen) :\n",
    "        UNK_subword = UNK_word_and_chr(sen)\n",
    "        txt += word_highligt_html(sen, UNK_subword, ['white']*len(UNK_subword),  ['#96C4ED']*len(UNK_subword)) + '<br/><br/>'\n",
    "        if count > 5:\n",
    "            break\n",
    "        count += 1\n",
    "HTML(txt)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  1%|          | 191/32470 [00:00<00:10, 2944.90it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "박용오(朴<span style=\"color: white; background-color:#96C4ED\">容</span><span style=\"color: white; background-color:#96C4ED\">旿</span>, 1937년 4월 29일(음력 3월 19일)(음력 3월 19일) ~ 2009년 11월 4일)는 서울에서 태어난 대한민국의 기업인으로 두산그룹 회장, KBO 총재 등을 역임했다.<br/><br/>2010년에는 아시아 가수 최초로 마이클 잭슨의 곡을 리메이크하였는데 당시 마이클 잭슨과 함께 작업했던 세계적인 뮤지션 스티브 <span style=\"color: white; background-color:#96C4ED\">바라캇</span>(Steve Barakatt)과 마이클 잭슨 곡 \"You are not alone\"을 작업해 화제가 되었다.<br/><br/>진도군은 진도개를 보기 위해 찾아온 관람객들에게 더욱 흥미롭고 즐거움을 선사하기 위해 ▲팔백리길을 돌아온 백구 생가 토피어리 조형물 ▲어로(<span style=\"color: white; background-color:#96C4ED\">犬</span>수영장)수렵장 ▲진도개 애견 캠핑장 등도 운영하고 있다.<br/><br/>백한성(白漢成, 水原<span style=\"color: white; background-color:#96C4ED\">鶴</span>人, 1899년 6월 15일 조선 충청도 공주 출생 ~ 1971년 10월 13일 대한민국 서울에서 별세.)은 대한민국의 정치가이며 법조인이다.<br/><br/>헌강왕(憲<span style=\"color: white; background-color:#96C4ED\">康</span>王, ~ 886년, 재위: 875년 ~ 886년)은 신라의 제49대 왕이다.<br/><br/>쇼니 씨(<span style=\"color: white; background-color:#96C4ED\"><span style=\"color: white; background-color:#96C4ED\">少</span></span><span style=\"color: white; background-color:#96C4ED\"><span style=\"color: white; background-color:#96C4ED\">弐</span></span>氏)의 8대 당주로 쇼니 요리히사(<span style=\"color: white; background-color:#96C4ED\"><span style=\"color: white; background-color:#96C4ED\">少</span></span><span style=\"color: white; background-color:#96C4ED\"><span style=\"color: white; background-color:#96C4ED\">弐</span></span><span style=\"color: white; background-color:#96C4ED\">頼</span><span style=\"color: white; background-color:#96C4ED\">尚</span>)의 둘째 아들이다.<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🛰️ Sentece UNK 확인"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "cnt = 1\n",
    "UNK_sentence_list = []\n",
    "for sen in tqdm(preprocessed_df['sentence']) :\n",
    "    if tokenizer.unk_token in tokenizer.tokenize(sen) :\n",
    "        UNK_sentence_list.extend(UNK_word_and_chr(sen))\n",
    "        cnt+=1\n",
    "print(cnt)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 32470/32470 [00:14<00:00, 2193.51it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2924\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "for idx, cont in enumerate(Counter(UNK_sentence_list).most_common(100)) :\n",
    "    if idx % 10 == 9 :\n",
    "        print()\n",
    "    else :\n",
    "        print(cont, end=\"\\t\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('李', 225)\t('崔', 60)\t('皇', 60)\t('后', 54)\t('永', 41)\t('尹', 38)\t('昌', 33)\t('慶', 30)\t('俊', 29)\t\n",
      "('趙', 25)\t('興', 24)\t('홋스퍼', 24)\t('孝', 23)\t('盧', 22)\t('承', 22)\t('梁', 22)\t('容', 21)\t('徐', 21)\t\n",
      "('熙', 21)\t('貞', 20)\t('沈', 20)\t('陵', 19)\t('鍾', 19)\t('錫', 18)\t('放', 18)\t('池', 18)\t('團', 18)\t\n",
      "('賢', 18)\t('洪', 18)\t('申', 17)\t('進', 17)\t('洙', 17)\t('泰', 17)\t('植', 16)\t('夏', 16)\t('秀', 16)\t\n",
      "('校', 16)\t('勳', 16)\t('吳', 16)\t('康', 15)\t('景', 15)\t('홋카이도', 15)\t('炳', 15)\t('恩', 15)\t('哲', 14)\t\n",
      "('羅', 14)\t('源', 14)\t('惠', 14)\t('範', 14)\t('榮', 14)\t('煥', 14)\t('宇', 14)\t('崇', 14)\t('少', 13)\t\n",
      "('忠', 13)\t('姬', 13)\t('숀', 13)\t('浩', 12)\t('嬪', 12)\t('根', 12)\t('唐', 12)\t('翁', 12)\t('鉉', 12)\t\n",
      "('勇', 12)\t('建', 12)\t('桓', 12)\t('玉', 11)\t('敬', 11)\t('淑', 11)\t('恭', 11)\t('智', 11)\t('宣', 11)\t\n",
      "('펭수', 10)\t('秋', 10)\t('樂', 10)\t('延', 10)\t('昭', 10)\t('順', 10)\t('奎', 10)\t('斗', 10)\t('應', 10)\t\n",
      "('베렝가리오', 10)\t('清', 10)\t('奉', 10)\t('藤', 10)\t('澤', 10)\t('閔', 10)\t('織', 10)\t('弐', 9)\t('泳', 9)\t\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "for_add = [token for token, cnt in Counter(UNK_sentence_list).items() if cnt >= 10]\n",
    "\n",
    "added_token_num = tokenizer.add_tokens(for_add)\n",
    "print(added_token_num)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "97\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🛰️ entity UNK 확인"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "subject_entity = []\n",
    "object_entity = []\n",
    "\n",
    "for i, j in zip(preprocessed_df['subject_entity'], preprocessed_df['object_entity']):\n",
    "    i = eval(i)['word']\n",
    "    j = eval(j)['word']\n",
    "\n",
    "    subject_entity.append(i)\n",
    "    object_entity.append(j)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "source": [
    "cnt = 1\n",
    "UNK_entity_list = []\n",
    "for token in tqdm(subject_entity+object_entity+t_subject_entity+t_object_entity) :\n",
    "    if tokenizer.unk_token in tokenizer.tokenize(token) :\n",
    "        char_unk = [UNK_word_and_chr(mor) for mor in mecab.morphs(token)]\n",
    "        UNK_entity_list.extend(chain(*char_unk))\n",
    "        cnt += 1\n",
    "print(cnt)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 80470/80470 [00:06<00:00, 12697.74it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "572\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "source": [
    "Counter(UNK_entity_list).most_common(100)\n",
    "for idx, cont in enumerate(Counter(UNK_entity_list).most_common(120)) :\n",
    "    if idx % 10 == 9 :\n",
    "        print()\n",
    "    else :\n",
    "        print(cont, end=\"\\t\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('홋스퍼', 28)\t('李', 12)\t('숀', 11)\t('홋카이도', 10)\t('에스파뇰', 10)\t('쥘', 9)\t('放', 9)\t('送', 9)\t('렝가리오', 8)\t\n",
      "('陵', 8)\t('織', 8)\t('쾰른', 7)\t('슝', 7)\t('리콴유', 6)\t('셴', 6)\t('묀헨글라트바흐', 6)\t('리셴녠', 5)\t('비욘세', 5)\t\n",
      "('弐', 5)\t('局', 5)\t('昌', 5)\t('梁', 5)\t('슌', 4)\t('흄', 4)\t('욘', 4)\t('푀', 4)\t('다롄', 4)\t\n",
      "('超', 4)\t('衛', 4)\t('弓', 4)\t('샨', 3)\t('쟝', 3)\t('뮐러', 3)\t('훙윈', 3)\t('푸르트벵글러', 3)\t('로퀜스', 3)\t\n",
      "('헴스워스', 3)\t('젬', 3)\t('슌지', 3)\t('꼰', 3)\t('에미넴', 3)\t('아녜스', 3)\t('훙', 3)\t('쓰촨', 3)\t('뎀', 3)\t\n",
      "('晋', 3)\t('宋', 3)\t('葛', 3)\t('后', 3)\t('聯', 3)\t('香', 3)\t('慶', 3)\t('윰', 3)\t('바라캇', 2)\t\n",
      "('녜', 2)\t('쳄부르스키', 2)\t('꽈드로스', 2)\t('래이쾨넨', 2)\t('뱌체슬라프', 2)\t('쥰', 2)\t('카뮈', 2)\t('귈', 2)\t('쳉', 2)\t\n",
      "('뮈르달', 2)\t('츨러', 2)\t('킵', 2)\t('젭', 2)\t('로바쳅스키', 2)\t('호엔촐레른', 2)\t('昶', 2)\t('쾨', 2)\t('퓌르트', 2)\t\n",
      "('맬', 2)\t('아르툠', 2)\t('촐라', 2)\t('채드윅', 2)\t('겅', 2)\t('똔텃투옛', 2)\t('앳킨슨', 2)\t('브뢴뷔', 2)\t('핼리팩스', 2)\t\n",
      "('켐니츠', 2)\t('伊', 2)\t('達', 2)\t('孝', 2)\t('廳', 2)\t('会', 2)\t('樹', 2)\t('郭', 2)\t('驥', 2)\t\n",
      "('干', 2)\t('永', 2)\t('乃', 2)\t('朗', 2)\t('麗', 2)\t('降', 2)\t('内', 2)\t('웡', 2)\t('洋', 2)\t\n",
      "('術', 2)\t('洪', 2)\t('熙', 2)\t('興', 2)\t('豊', 2)\t('尹', 2)\t('鍾', 2)\t('煦', 2)\t('柳', 2)\t\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "source": [
    "for_add = [token for token, cnt in Counter(UNK_entity_list).items() if cnt > 2]\n",
    "\n",
    "added_token_num = tokenizer.add_tokens(for_add)\n",
    "print(added_token_num)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 🛰️시험"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "Test_csv_file = '/opt/ml/dataset/test/test_data.csv'\n",
    "test_df = pd.read_csv(Test_csv_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "t_subject_entity = []\n",
    "t_object_entity = []\n",
    "\n",
    "for i, j in zip(test_df['subject_entity'], test_df['object_entity']):\n",
    "    i = eval(i)['word']\n",
    "    j = eval(j)['word']\n",
    "\n",
    "    t_subject_entity.append(i)\n",
    "    t_object_entity.append(j)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "source": [
    "cnt = 1\n",
    "t_UNK_entity_list = []\n",
    "for sen in tqdm(t_subject_entity+t_object_entity) :\n",
    "    if tokenizer.unk_token in tokenizer.tokenize(sen) :\n",
    "        t_UNK_entity_list.extend(UNK_word_and_chr(sen))\n",
    "        cnt += 1\n",
    "print(cnt)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 15530/15530 [00:01<00:00, 12986.41it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "99\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "source": [
    "print(Counter(t_UNK_entity_list).most_common())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('李', 5), ('윰댕', 3), ('숀', 3), ('묀헨글라트바흐', 2), ('葛', 2), ('衛', 2), ('秀', 2), ('雄', 2), ('에스파뇰', 2), ('宋', 2), ('맬컴', 2), ('姜', 2), ('쥘', 2), ('펭수', 2), ('守', 1), ('벵어', 1), ('帶', 1), ('慶', 1), ('應', 1), ('도스토옙스키', 1), ('엔지켐생명과학', 1), ('丸', 1), ('彫', 1), ('헴스워스', 1), ('滿', 1), ('빕스', 1), ('오뎀윙기', 1), ('크뢸루프', 1), ('沙', 1), ('梁', 1), ('웡', 1), ('바르뎀', 1), ('閔', 1), ('綾', 1), ('皓', 1), ('황페이훙', 1), ('沈', 1), ('御', 1), ('陵', 1), ('珥', 1), ('포로셴코', 1), ('簾', 1), ('에미넴', 1), ('溪', 1), ('브륀힐드', 1), ('寧', 1), ('로케푀이', 1), ('우젠슝', 1), ('朔', 1), ('趙', 1), ('妃', 1), ('尹', 1), ('潽', 1), ('쑨시엔위', 1), ('昌', 1), ('仇', 1), ('牙', 1), ('Perišić', 1), ('必', 1), ('琪', 1), ('바츨라프', 1), ('쿠샨', 1), ('段', 1), ('龕', 1), ('址', 1), ('鎔', 1), ('쑨원', 1), ('응우옌반냑', 1), ('쑨양', 1), ('尙', 1), ('桓', 1), ('흄', 1), ('諸', 1), ('亮', 1), ('臨', 1), ('홋스퍼', 1), ('일리리쿰', 1), ('菅', 1), ('偉', 1), ('斌', 1), ('朗', 1), ('顯', 1), ('后', 1), ('袁', 1), ('譚', 1), ('惠', 1), ('局', 1), ('쟌느', 1), ('亨', 1), ('로퀜스', 1), ('뿡뿡이', 1), ('됭케르크', 1), ('펨미닐레', 1), ('채드윅', 1), ('曹', 1), ('가믈리', 1), ('둠밈', 1), ('莊', 1), ('빌햘름', 1), ('遇', 1), ('睦', 1), ('苞', 1), ('비욘드', 1), ('스킵', 1), ('아르카숑', 1)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 🛸 한 단어로 이루어지지 않은 entity\n",
    "- 여러 단어\n",
    "- 특정 패턴이 존재하는\n",
    "- 맞추기 어려울 것 같은\n",
    "+ 위 내용을 만족하는 entity는 entity token wrap 해주는게 어떠려나?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "for obj in df['object_entity'] :\n",
    "    d_obj = eval(obj)\n",
    "    if d_obj['type'] == 'DAT' :\n",
    "        if len(d_obj['word'].split()) > 1 :\n",
    "            print(obj)\n",
    "            test_encoced = tokenizer.encode(d_obj['word'])\n",
    "            print(test_encoced)\n",
    "            Id_to_tokens = tokenizer.convert_ids_to_tokens(test_encoced[1:-1])\n",
    "            print(Id_to_tokens)\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'word': '1937년 4월 29일', 'start_idx': 9, 'end_idx': 20, 'type': 'DAT'}\n",
      "[0, 20533, 2440, 24, 2429, 4346, 2210, 2]\n",
      "['1937', '##년', '4', '##월', '29', '##일']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}