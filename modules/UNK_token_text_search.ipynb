{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from typing import  Dict, List, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-10-02 23:08:58.688222: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "Train_csv_file = '/opt/ml/dataset/train/train.csv'\n",
    "Test_csv_file = '/opt/ml/dataset/test/test_data.csv'\n",
    "\n",
    "df = pd.read_csv(Train_csv_file)\n",
    "df = df.drop_duplicates(['sentence','subject_entity','object_entity','label']).reset_index()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "MODEL_NAME = \"klue/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "vocab = list(tokenizer.get_vocab().keys())\n",
    "unused_list = [word for word in vocab if word.startswith('[unused')]        \n",
    "print(f'unused token count: {len(unused_list)}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "unused token count: 500\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ğŸ›°ï¸ ì „ì²˜ë¦¬"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def remove_special_char(sentence):\n",
    "    \"\"\" íŠ¹ìˆ˜ë¬¸ì ë° ë…ì¼ì–´ ì œê±°, ìˆ˜ì •\"\"\"\n",
    "    sentence = re.sub(r'[Ã€-Ã¿]+','', sentence) # ë…ì¼ì–´\n",
    "    sentence = re.sub(r'[\\u0600-\\u06FF]+','', sentence)  # ì‚¬ìš°ë””ì–´\n",
    "    sentence = re.sub(r'[\\u00C0-\\u02B0]+','', sentence)  # ë¼í‹´ì–´\n",
    "    sentence = re.sub(r'[ÃŸâ†”â’¶Ø¨â‚¬â˜â˜Â±âˆ]+','', sentence)\n",
    "    sentence = re.sub('â€“','â”€', sentence)\n",
    "    sentence = re.sub('âŸª','ã€Š', sentence)\n",
    "    sentence = re.sub('âŸ«','ã€‹', sentence)\n",
    "    sentence = re.sub('ï½¥','ãƒ»', sentence)\n",
    "    sentence = re.sub('Âµ','â„“', sentence)\n",
    "    sentence = re.sub('Â®','ãˆœ', sentence)\n",
    "    sentence = re.sub('ï½','ãˆœ', sentence)\n",
    "    return sentence\n",
    "\n",
    "test_sen = 'Hermann MÃ¼ller'\n",
    "remove_special_char(test_sen)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Hermann Mller'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def add_space_char(sentence) :\n",
    "    def add_space(match) :\n",
    "        res_str = ', '.join(match.group().split(',')).rstrip()\n",
    "        return res_str\n",
    "    p = re.compile(r'([ê¸°-í£\\w\\-]+,)+[ê¸°-í£\\w\\-]+')\n",
    "    sentence = p.sub(add_space, sentence)\n",
    "    return sentence\n",
    "test_sen = 'ì•¨ë²”ì—ëŠ” ì—ë¯¸ë„´,G-Unit,ë‹¥í„°ë“œë ˆ,ì œì´ë¯¸ í­ìŠ¤ ë“±ì´ ì°¸ì—¬í•˜ê³ ,ì˜êµ­ì°¨íŠ¸ì—ì„œë„ 1ìœ„ë¥¼ í•œë‹¤.'\n",
    "add_space_char(test_sen)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ì•¨ë²”ì—ëŠ” ì—ë¯¸ë„´, G-Unit, ë‹¥í„°ë“œë ˆ, ì œì´ë¯¸ í­ìŠ¤ ë“±ì´ ì°¸ì—¬í•˜ê³ , ì˜êµ­ì°¨íŠ¸ì—ì„œë„ 1ìœ„ë¥¼ í•œë‹¤.'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def substitution_date(sentence):\n",
    "    \"\"\"\n",
    "    ê¸°ê°„ í‘œì‹œ '-' => '~'\n",
    "    1223ë…„ â€“ => 1223ë…„ ~ \n",
    "    \"\"\"\n",
    "    def sub_tibble(match) :\n",
    "        res_str = re.sub('[â€“\\-]','~',match.group())\n",
    "        return res_str\n",
    "    re_patterns = [\n",
    "        r'(\\d{2,4}ë…„\\s*)(\\d{1,2}[ì›”|ì¼]\\s*)(\\d{1,2}[ì›”|ì¼])\\s*[â€“\\-]',\n",
    "        r'(\\d{2,4}ë…„\\s*)(\\d{1,2}[ì›”|ì¼]\\s*)\\s*[â€“\\-]',\n",
    "        r'(\\d{2,4}ë…„\\s*)\\s*[â€“\\-]',\n",
    "        r'\\((\\d{4}[â€“\\-]\\d{2,4})\\)'\n",
    "    ]\n",
    "    for re_pattern in re_patterns :\n",
    "        p = re.compile(re_pattern)\n",
    "        sentence = p.sub(sub_tibble, sentence)   \n",
    "    return sentence\n",
    "\n",
    "test_sen = 'í›„ ì‹œë‹ˆì–´ ëŒ€íšŒë¡œ ì˜¬ë¼ê°€ì„œ (1934â€“1942) ì‹œì¦Œ ISU ì‡¼íŠ¸íŠ¸ë™ ì›”ë“œì»µì— 4ì°¨ë¡€ ì¶œì „í•˜ì—¬ ê¸ˆë©”ë‹¬ 4ê°œ, ì€ë©”ë‹¬ 5ê°œë¥¼ ì°¨ì§€í•˜ì˜€ìœ¼ë©°, 2013-14 ì‹œì¦Œì—ì„œ ê¹€ì•„ë‘ì€ ì‡¼íŠ¸íŠ¸ë™ ìŠ¤í”¼ë“œ'\n",
    "substitution_date(test_sen)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'í›„ ì‹œë‹ˆì–´ ëŒ€íšŒë¡œ ì˜¬ë¼ê°€ì„œ (1934~1942) ì‹œì¦Œ ISU ì‡¼íŠ¸íŠ¸ë™ ì›”ë“œì»µì— 4ì°¨ë¡€ ì¶œì „í•˜ì—¬ ê¸ˆë©”ë‹¬ 4ê°œ, ì€ë©”ë‹¬ 5ê°œë¥¼ ì°¨ì§€í•˜ì˜€ìœ¼ë©°, 2013-14 ì‹œì¦Œì—ì„œ ê¹€ì•„ë‘ì€ ì‡¼íŠ¸íŠ¸ë™ ìŠ¤í”¼ë“œ'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def add_space_year(sentence):\n",
    "    \"\"\"\n",
    "    ìˆ«ìì™€ ë…„ ì‚¬ì´ì— ê³µë°±\n",
    "    1223ë…„ => 1223 ë…„ => â‘¦ ë…„\n",
    "    \"\"\"\n",
    "    def add_space(match) :\n",
    "        # res_str = 'â‘¦ ' + match.group()[4:]\n",
    "        res_str =  match.group()[:4] +' ' + match.group()[4:]\n",
    "        return res_str\n",
    "    p = re.compile(r'\\d{4}ë…„')\n",
    "    sentence = p.sub(add_space, sentence)\n",
    "    return sentence\n",
    "test_sen = '2010ë…„ì—ëŠ” ì•„ì‹œì•„ ê°€ìˆ˜ ìµœì´ˆë¡œ ë§ˆì´í´ ì­ìŠ¨ì˜ ê³¡ì„ ë¦¬ë©”ì´í¬í•˜ì˜€ëŠ”ë°'\n",
    "add_space_year(test_sen)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2010 ë…„ì—ëŠ” ì•„ì‹œì•„ ê°€ìˆ˜ ìµœì´ˆë¡œ ë§ˆì´í´ ì­ìŠ¨ì˜ ê³¡ì„ ë¦¬ë©”ì´í¬í•˜ì˜€ëŠ”ë°'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def preprocessing(sentence) :\n",
    "    sent = remove_special_char(sentence)\n",
    "    sent = substitution_date(sent)\n",
    "    # sent = add_space_year(sent)\n",
    "    sent = add_space_char(sent)\n",
    "    return sent"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "df = pd.read_csv(Train_csv_file)\n",
    "\n",
    "ess = ['sentence','subject_entity','object_entity']\n",
    "preprocessed_df = df.copy()\n",
    "for col in ess :\n",
    "    preprocessed_df[col] = preprocessed_df[col].apply(preprocessing)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ğŸ›°ï¸UNKìœ¼ë¡œ ë³€í•˜ëŠ” word ë° char í™•ì¸"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from IPython.core.display import HTML\n",
    "def word_highligt_html(txt, word, color='black', highlight=None, attr=None):\n",
    "    if isinstance(word, str):\n",
    "        txt = txt.replace(word, f'<span style=\"color: {color}; background-color:{highlight}\">{word}</span>')\n",
    "    else:\n",
    "        if not isinstance(color, list):\n",
    "            color = [color] * len(word)\n",
    "        if not isinstance(highlight, list):\n",
    "            highlight = [highlight] * len(word)\n",
    "        for w, c, h in zip(word, color, highlight):\n",
    "            txt = txt.replace(w, f'<span style=\"color: {c}; background-color:{h}\">{w}</span>')\n",
    "    return txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def subword_parsing(wordpiece:List) -> List[str]: ## subword # ì œê±°ìš©\n",
    "    Known_char = []\n",
    "    for subword in wordpiece :\n",
    "        if subword == tokenizer.unk_token :\n",
    "            Known_char.append(tokenizer.unk_token)\n",
    "        else :\n",
    "            string = subword.replace('#','')\n",
    "            Known_char.extend(string)\n",
    "    return Known_char\n",
    "\n",
    "\n",
    "def UNK_word_and_chr(text:str) -> Tuple[List[str], List[str]]:\n",
    "    sub_word_UNK_list = []\n",
    "    \n",
    "    def add_space(match) :\n",
    "        bracket = match.group()\n",
    "        added = ' ' + bracket + ' '\n",
    "        return added\n",
    "    p = re.compile(r'[\\([)\\]|,|-|~|-|â€˜|â€™|\"|\\']')\n",
    "    words_list = p.sub(add_space, text).split()\n",
    "    for word in words_list :\n",
    "        subwordpieces_ID_encoded = tokenizer.tokenize(word)\n",
    "        Known_subword = subword_parsing(subwordpieces_ID_encoded)\n",
    "        for sub_char, NK_char in zip(word, Known_subword) :\n",
    "            if sub_char != NK_char and len(word) == len(Known_subword) :\n",
    "                sub_word_UNK_list.append(sub_char)\n",
    "            elif sub_char != NK_char and len(word) != len(Known_subword) :\n",
    "                sub_word_UNK_list.append(word)\n",
    "                break\n",
    "    return sub_word_UNK_list\n",
    "    \n",
    "text ='ë°•ìš©ì˜¤(æœ´å®¹æ—¿, 1937ë…„ 4ì›” 29ì¼(ìŒë ¥ 3ì›” 19ì¼)(ìŒë ¥ 3ì›” 19ì¼) ~ 2009ë…„ 11ì›” 4ì¼)ëŠ” ì„œìš¸ì—ì„œ íƒœì–´ë‚œ ëŒ€í•œë¯¼êµ­ì˜ ê¸°ì—…ì¸ìœ¼ë¡œ ë‘ì‚°ê·¸ë£¹ íšŒì¥, KBO ì´ì¬ ë“±ì„ ì—­ì„í–ˆë‹¤.'\n",
    "print(UNK_word_and_chr(text))\n",
    "if tokenizer.unk_token in tokenizer.tokenize(text) :\n",
    "    print(tokenizer.tokenize(text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['å®¹', 'æ—¿']\n",
      "['ë°•ìš©', '##ì˜¤', '(', 'æœ´', '[UNK]', '[UNK]', ',', '1937', '##ë…„', '4', '##ì›”', '29', '##ì¼', '(', 'ìŒë ¥', '3', '##ì›”', '19', '##ì¼', ')', '(', 'ìŒë ¥', '3', '##ì›”', '19', '##ì¼', ')', '~', '2009', '##ë…„', '11', '##ì›”', '4', '##ì¼', ')', 'ëŠ”', 'ì„œìš¸', '##ì—ì„œ', 'íƒœì–´ë‚œ', 'ëŒ€í•œë¯¼êµ­', '##ì˜', 'ê¸°ì—…ì¸', '##ìœ¼ë¡œ', 'ë‘ì‚°', '##ê·¸ë£¹', 'íšŒì¥', ',', 'KBO', 'ì´ì¬', 'ë“±', '##ì„', 'ì—­ì„', '##í–ˆ', '##ë‹¤', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ğŸ›°ï¸ UNK ë¶„í¬"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "txt = ''\n",
    "count = 1\n",
    "unk_len = 0\n",
    "for sen in tqdm(preprocessed_df['sentence']) :\n",
    "    if tokenizer.unk_token in tokenizer.tokenize(sen) :\n",
    "        UNK_subword = UNK_word_and_chr(sen)\n",
    "        txt += word_highligt_html(sen, UNK_subword, ['white']*len(UNK_subword),  ['#96C4ED']*len(UNK_subword)) + '<br/><br/>'\n",
    "        if count > 5:\n",
    "            break\n",
    "        count += 1\n",
    "HTML(txt)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  1%|          | 191/32470 [00:00<00:10, 2944.90it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "ë°•ìš©ì˜¤(æœ´<span style=\"color: white; background-color:#96C4ED\">å®¹</span><span style=\"color: white; background-color:#96C4ED\">æ—¿</span>, 1937ë…„ 4ì›” 29ì¼(ìŒë ¥ 3ì›” 19ì¼)(ìŒë ¥ 3ì›” 19ì¼) ~ 2009ë…„ 11ì›” 4ì¼)ëŠ” ì„œìš¸ì—ì„œ íƒœì–´ë‚œ ëŒ€í•œë¯¼êµ­ì˜ ê¸°ì—…ì¸ìœ¼ë¡œ ë‘ì‚°ê·¸ë£¹ íšŒì¥, KBO ì´ì¬ ë“±ì„ ì—­ì„í–ˆë‹¤.<br/><br/>2010ë…„ì—ëŠ” ì•„ì‹œì•„ ê°€ìˆ˜ ìµœì´ˆë¡œ ë§ˆì´í´ ì­ìŠ¨ì˜ ê³¡ì„ ë¦¬ë©”ì´í¬í•˜ì˜€ëŠ”ë° ë‹¹ì‹œ ë§ˆì´í´ ì­ìŠ¨ê³¼ í•¨ê»˜ ì‘ì—…í–ˆë˜ ì„¸ê³„ì ì¸ ë®¤ì§€ì…˜ ìŠ¤í‹°ë¸Œ <span style=\"color: white; background-color:#96C4ED\">ë°”ë¼ìº‡</span>(Steve Barakatt)ê³¼ ë§ˆì´í´ ì­ìŠ¨ ê³¡ \"You are not alone\"ì„ ì‘ì—…í•´ í™”ì œê°€ ë˜ì—ˆë‹¤.<br/><br/>ì§„ë„êµ°ì€ ì§„ë„ê°œë¥¼ ë³´ê¸° ìœ„í•´ ì°¾ì•„ì˜¨ ê´€ëŒê°ë“¤ì—ê²Œ ë”ìš± í¥ë¯¸ë¡­ê³  ì¦ê±°ì›€ì„ ì„ ì‚¬í•˜ê¸° ìœ„í•´ â–²íŒ”ë°±ë¦¬ê¸¸ì„ ëŒì•„ì˜¨ ë°±êµ¬ ìƒê°€ í† í”¼ì–´ë¦¬ ì¡°í˜•ë¬¼ â–²ì–´ë¡œ(<span style=\"color: white; background-color:#96C4ED\">çŠ¬</span>ìˆ˜ì˜ì¥)ìˆ˜ë µì¥ â–²ì§„ë„ê°œ ì• ê²¬ ìº í•‘ì¥ ë“±ë„ ìš´ì˜í•˜ê³  ìˆë‹¤.<br/><br/>ë°±í•œì„±(ç™½æ¼¢æˆ, æ°´åŸ<span style=\"color: white; background-color:#96C4ED\">é¶´</span>äºº, 1899ë…„ 6ì›” 15ì¼ ì¡°ì„  ì¶©ì²­ë„ ê³µì£¼ ì¶œìƒ ~ 1971ë…„ 10ì›” 13ì¼ ëŒ€í•œë¯¼êµ­ ì„œìš¸ì—ì„œ ë³„ì„¸.)ì€ ëŒ€í•œë¯¼êµ­ì˜ ì •ì¹˜ê°€ì´ë©° ë²•ì¡°ì¸ì´ë‹¤.<br/><br/>í—Œê°•ì™•(æ†²<span style=\"color: white; background-color:#96C4ED\">åº·</span>ç‹, ~ 886ë…„, ì¬ìœ„: 875ë…„ ~ 886ë…„)ì€ ì‹ ë¼ì˜ ì œ49ëŒ€ ì™•ì´ë‹¤.<br/><br/>ì‡¼ë‹ˆ ì”¨(<span style=\"color: white; background-color:#96C4ED\"><span style=\"color: white; background-color:#96C4ED\">å°‘</span></span><span style=\"color: white; background-color:#96C4ED\"><span style=\"color: white; background-color:#96C4ED\">å¼</span></span>æ°)ì˜ 8ëŒ€ ë‹¹ì£¼ë¡œ ì‡¼ë‹ˆ ìš”ë¦¬íˆì‚¬(<span style=\"color: white; background-color:#96C4ED\"><span style=\"color: white; background-color:#96C4ED\">å°‘</span></span><span style=\"color: white; background-color:#96C4ED\"><span style=\"color: white; background-color:#96C4ED\">å¼</span></span><span style=\"color: white; background-color:#96C4ED\">é ¼</span><span style=\"color: white; background-color:#96C4ED\">å°š</span>)ì˜ ë‘˜ì§¸ ì•„ë“¤ì´ë‹¤.<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ›°ï¸ Sentece UNK í™•ì¸"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "cnt = 1\n",
    "UNK_sentence_list = []\n",
    "for sen in tqdm(preprocessed_df['sentence']) :\n",
    "    if tokenizer.unk_token in tokenizer.tokenize(sen) :\n",
    "        UNK_sentence_list.extend(UNK_word_and_chr(sen))\n",
    "        cnt+=1\n",
    "print(cnt)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32470/32470 [00:14<00:00, 2193.51it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2924\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "for idx, cont in enumerate(Counter(UNK_sentence_list).most_common(100)) :\n",
    "    if idx % 10 == 9 :\n",
    "        print()\n",
    "    else :\n",
    "        print(cont, end=\"\\t\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('æ', 225)\t('å´”', 60)\t('çš‡', 60)\t('å', 54)\t('æ°¸', 41)\t('å°¹', 38)\t('æ˜Œ', 33)\t('æ…¶', 30)\t('ä¿Š', 29)\t\n",
      "('è¶™', 25)\t('èˆˆ', 24)\t('í™‹ìŠ¤í¼', 24)\t('å­', 23)\t('ç›§', 22)\t('æ‰¿', 22)\t('æ¢', 22)\t('å®¹', 21)\t('å¾', 21)\t\n",
      "('ç†™', 21)\t('è²', 20)\t('æ²ˆ', 20)\t('é™µ', 19)\t('é¾', 19)\t('éŒ«', 18)\t('æ”¾', 18)\t('æ± ', 18)\t('åœ˜', 18)\t\n",
      "('è³¢', 18)\t('æ´ª', 18)\t('ç”³', 17)\t('é€²', 17)\t('æ´™', 17)\t('æ³°', 17)\t('æ¤', 16)\t('å¤', 16)\t('ç§€', 16)\t\n",
      "('æ ¡', 16)\t('å‹³', 16)\t('å³', 16)\t('åº·', 15)\t('æ™¯', 15)\t('í™‹ì¹´ì´ë„', 15)\t('ç‚³', 15)\t('æ©', 15)\t('å“²', 14)\t\n",
      "('ç¾…', 14)\t('æº', 14)\t('æƒ ', 14)\t('ç¯„', 14)\t('æ¦®', 14)\t('ç…¥', 14)\t('å®‡', 14)\t('å´‡', 14)\t('å°‘', 13)\t\n",
      "('å¿ ', 13)\t('å§¬', 13)\t('ìˆ€', 13)\t('æµ©', 12)\t('å¬ª', 12)\t('æ ¹', 12)\t('å”', 12)\t('ç¿', 12)\t('é‰‰', 12)\t\n",
      "('å‹‡', 12)\t('å»º', 12)\t('æ¡“', 12)\t('ç‰', 11)\t('æ•¬', 11)\t('æ·‘', 11)\t('æ­', 11)\t('æ™º', 11)\t('å®£', 11)\t\n",
      "('í­ìˆ˜', 10)\t('ç§‹', 10)\t('æ¨‚', 10)\t('å»¶', 10)\t('æ˜­', 10)\t('é †', 10)\t('å¥', 10)\t('æ–—', 10)\t('æ‡‰', 10)\t\n",
      "('ë² ë ê°€ë¦¬ì˜¤', 10)\t('æ¸…', 10)\t('å¥‰', 10)\t('è—¤', 10)\t('æ¾¤', 10)\t('é–”', 10)\t('ç¹”', 10)\t('å¼', 9)\t('æ³³', 9)\t\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "for_add = [token for token, cnt in Counter(UNK_sentence_list).items() if cnt >= 10]\n",
    "\n",
    "added_token_num = tokenizer.add_tokens(for_add)\n",
    "print(added_token_num)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "97\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ›°ï¸ entity UNK í™•ì¸"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "subject_entity = []\n",
    "object_entity = []\n",
    "\n",
    "for i, j in zip(preprocessed_df['subject_entity'], preprocessed_df['object_entity']):\n",
    "    i = eval(i)['word']\n",
    "    j = eval(j)['word']\n",
    "\n",
    "    subject_entity.append(i)\n",
    "    object_entity.append(j)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "source": [
    "cnt = 1\n",
    "UNK_entity_list = []\n",
    "for token in tqdm(subject_entity+object_entity+t_subject_entity+t_object_entity) :\n",
    "    if tokenizer.unk_token in tokenizer.tokenize(token) :\n",
    "        char_unk = [UNK_word_and_chr(mor) for mor in mecab.morphs(token)]\n",
    "        UNK_entity_list.extend(chain(*char_unk))\n",
    "        cnt += 1\n",
    "print(cnt)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80470/80470 [00:06<00:00, 12697.74it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "572\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "source": [
    "Counter(UNK_entity_list).most_common(100)\n",
    "for idx, cont in enumerate(Counter(UNK_entity_list).most_common(120)) :\n",
    "    if idx % 10 == 9 :\n",
    "        print()\n",
    "    else :\n",
    "        print(cont, end=\"\\t\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('í™‹ìŠ¤í¼', 28)\t('æ', 12)\t('ìˆ€', 11)\t('í™‹ì¹´ì´ë„', 10)\t('ì—ìŠ¤íŒŒë‡°', 10)\t('ì¥˜', 9)\t('æ”¾', 9)\t('é€', 9)\t('ë ê°€ë¦¬ì˜¤', 8)\t\n",
      "('é™µ', 8)\t('ç¹”', 8)\t('ì¾°ë¥¸', 7)\t('ìŠ', 7)\t('ë¦¬ì½´ìœ ', 6)\t('ì…´', 6)\t('ë¬€í—¨ê¸€ë¼íŠ¸ë°”í', 6)\t('ë¦¬ì…´ë… ', 5)\t('ë¹„ìš˜ì„¸', 5)\t\n",
      "('å¼', 5)\t('å±€', 5)\t('æ˜Œ', 5)\t('æ¢', 5)\t('ìŠŒ', 4)\t('í„', 4)\t('ìš˜', 4)\t('í‘€', 4)\t('ë‹¤ë¡„', 4)\t\n",
      "('è¶…', 4)\t('è¡›', 4)\t('å¼“', 4)\t('ìƒ¨', 3)\t('ìŸ', 3)\t('ë®ëŸ¬', 3)\t('í›™ìœˆ', 3)\t('í‘¸ë¥´íŠ¸ë²µê¸€ëŸ¬', 3)\t('ë¡œí€œìŠ¤', 3)\t\n",
      "('í—´ìŠ¤ì›ŒìŠ¤', 3)\t('ì ¬', 3)\t('ìŠŒì§€', 3)\t('ê¼°', 3)\t('ì—ë¯¸ë„´', 3)\t('ì•„ë…œìŠ¤', 3)\t('í›™', 3)\t('ì“°ì´¨', 3)\t('ë€', 3)\t\n",
      "('æ™‹', 3)\t('å®‹', 3)\t('è‘›', 3)\t('å', 3)\t('è¯', 3)\t('é¦™', 3)\t('æ…¶', 3)\t('ìœ°', 3)\t('ë°”ë¼ìº‡', 2)\t\n",
      "('ë…œ', 2)\t('ì³„ë¶€ë¥´ìŠ¤í‚¤', 2)\t('ê½ˆë“œë¡œìŠ¤', 2)\t('ë˜ì´ì¾¨ë„¨', 2)\t('ë±Œì²´ìŠ¬ë¼í”„', 2)\t('ì¥°', 2)\t('ì¹´ë®ˆ', 2)\t('ê·ˆ', 2)\t('ì³‰', 2)\t\n",
      "('ë®ˆë¥´ë‹¬', 2)\t('ì¸¨ëŸ¬', 2)\t('í‚µ', 2)\t('ì ­', 2)\t('ë¡œë°”ì³…ìŠ¤í‚¤', 2)\t('í˜¸ì—”ì´ë ˆë¥¸', 2)\t('æ˜¶', 2)\t('ì¾¨', 2)\t('í“Œë¥´íŠ¸', 2)\t\n",
      "('ë§¬', 2)\t('ì•„ë¥´íˆ ', 2)\t('ì´ë¼', 2)\t('ì±„ë“œìœ…', 2)\t('ê²…', 2)\t('ë˜”í…ƒíˆ¬ì˜›', 2)\t('ì•³í‚¨ìŠ¨', 2)\t('ë¸Œë¢´ë·”', 2)\t('í•¼ë¦¬íŒ©ìŠ¤', 2)\t\n",
      "('ì¼ë‹ˆì¸ ', 2)\t('ä¼Š', 2)\t('é”', 2)\t('å­', 2)\t('å»³', 2)\t('ä¼š', 2)\t('æ¨¹', 2)\t('éƒ­', 2)\t('é©¥', 2)\t\n",
      "('å¹²', 2)\t('æ°¸', 2)\t('ä¹ƒ', 2)\t('æœ—', 2)\t('éº—', 2)\t('é™', 2)\t('å†…', 2)\t('ì›¡', 2)\t('æ´‹', 2)\t\n",
      "('è¡“', 2)\t('æ´ª', 2)\t('ç†™', 2)\t('èˆˆ', 2)\t('è±Š', 2)\t('å°¹', 2)\t('é¾', 2)\t('ç…¦', 2)\t('æŸ³', 2)\t\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "source": [
    "for_add = [token for token, cnt in Counter(UNK_entity_list).items() if cnt > 2]\n",
    "\n",
    "added_token_num = tokenizer.add_tokens(for_add)\n",
    "print(added_token_num)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ğŸ›°ï¸ì‹œí—˜"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "Test_csv_file = '/opt/ml/dataset/test/test_data.csv'\n",
    "test_df = pd.read_csv(Test_csv_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "t_subject_entity = []\n",
    "t_object_entity = []\n",
    "\n",
    "for i, j in zip(test_df['subject_entity'], test_df['object_entity']):\n",
    "    i = eval(i)['word']\n",
    "    j = eval(j)['word']\n",
    "\n",
    "    t_subject_entity.append(i)\n",
    "    t_object_entity.append(j)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "source": [
    "cnt = 1\n",
    "t_UNK_entity_list = []\n",
    "for sen in tqdm(t_subject_entity+t_object_entity) :\n",
    "    if tokenizer.unk_token in tokenizer.tokenize(sen) :\n",
    "        t_UNK_entity_list.extend(UNK_word_and_chr(sen))\n",
    "        cnt += 1\n",
    "print(cnt)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15530/15530 [00:01<00:00, 12986.41it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "99\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "source": [
    "print(Counter(t_UNK_entity_list).most_common())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('æ', 5), ('ìœ°ëŒ•', 3), ('ìˆ€', 3), ('ë¬€í—¨ê¸€ë¼íŠ¸ë°”í', 2), ('è‘›', 2), ('è¡›', 2), ('ç§€', 2), ('é›„', 2), ('ì—ìŠ¤íŒŒë‡°', 2), ('å®‹', 2), ('ë§¬ì»´', 2), ('å§œ', 2), ('ì¥˜', 2), ('í­ìˆ˜', 2), ('å®ˆ', 1), ('ë²µì–´', 1), ('å¸¶', 1), ('æ…¶', 1), ('æ‡‰', 1), ('ë„ìŠ¤í† ì˜™ìŠ¤í‚¤', 1), ('ì—”ì§€ì¼ìƒëª…ê³¼í•™', 1), ('ä¸¸', 1), ('å½«', 1), ('í—´ìŠ¤ì›ŒìŠ¤', 1), ('æ»¿', 1), ('ë¹•ìŠ¤', 1), ('ì˜¤ë€ìœ™ê¸°', 1), ('í¬ë¢¸ë£¨í”„', 1), ('æ²™', 1), ('æ¢', 1), ('ì›¡', 1), ('ë°”ë¥´ë€', 1), ('é–”', 1), ('ç¶¾', 1), ('çš“', 1), ('í™©í˜ì´í›™', 1), ('æ²ˆ', 1), ('å¾¡', 1), ('é™µ', 1), ('ç¥', 1), ('í¬ë¡œì…´ì½”', 1), ('ç°¾', 1), ('ì—ë¯¸ë„´', 1), ('æºª', 1), ('ë¸Œë¥€íë“œ', 1), ('å¯§', 1), ('ë¡œì¼€í‘€ì´', 1), ('ìš°ì  ìŠ', 1), ('æœ”', 1), ('è¶™', 1), ('å¦ƒ', 1), ('å°¹', 1), ('æ½½', 1), ('ì‘¨ì‹œì—”ìœ„', 1), ('æ˜Œ', 1), ('ä»‡', 1), ('ç‰™', 1), ('PeriÅ¡iÄ‡', 1), ('å¿…', 1), ('çª', 1), ('ë°”ì¸¨ë¼í”„', 1), ('ì¿ ìƒ¨', 1), ('æ®µ', 1), ('é¾•', 1), ('å€', 1), ('é”', 1), ('ì‘¨ì›', 1), ('ì‘ìš°ì˜Œë°˜ëƒ‘', 1), ('ì‘¨ì–‘', 1), ('å°™', 1), ('æ¡“', 1), ('í„', 1), ('è«¸', 1), ('äº®', 1), ('è‡¨', 1), ('í™‹ìŠ¤í¼', 1), ('ì¼ë¦¬ë¦¬ì¿°', 1), ('è…', 1), ('å‰', 1), ('æ–Œ', 1), ('æœ—', 1), ('é¡¯', 1), ('å', 1), ('è¢', 1), ('è­š', 1), ('æƒ ', 1), ('å±€', 1), ('ìŸŒëŠ', 1), ('äº¨', 1), ('ë¡œí€œìŠ¤', 1), ('ë¿¡ë¿¡ì´', 1), ('ë­ì¼€ë¥´í¬', 1), ('í¨ë¯¸ë‹ë ˆ', 1), ('ì±„ë“œìœ…', 1), ('æ›¹', 1), ('ê°€ë¯ˆë¦¬', 1), ('ë‘ ë°ˆ', 1), ('èŠ', 1), ('ë¹Œí–˜ë¦„', 1), ('é‡', 1), ('ç¦', 1), ('è‹', 1), ('ë¹„ìš˜ë“œ', 1), ('ìŠ¤í‚µ', 1), ('ì•„ë¥´ì¹´ìˆ‘', 1)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ğŸ›¸ í•œ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§€ì§€ ì•Šì€ entity\n",
    "- ì—¬ëŸ¬ ë‹¨ì–´\n",
    "- íŠ¹ì • íŒ¨í„´ì´ ì¡´ì¬í•˜ëŠ”\n",
    "- ë§ì¶”ê¸° ì–´ë ¤ìš¸ ê²ƒ ê°™ì€\n",
    "+ ìœ„ ë‚´ìš©ì„ ë§Œì¡±í•˜ëŠ” entityëŠ” entity token wrap í•´ì£¼ëŠ”ê²Œ ì–´ë– ë ¤ë‚˜?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "for obj in df['object_entity'] :\n",
    "    d_obj = eval(obj)\n",
    "    if d_obj['type'] == 'DAT' :\n",
    "        if len(d_obj['word'].split()) > 1 :\n",
    "            print(obj)\n",
    "            test_encoced = tokenizer.encode(d_obj['word'])\n",
    "            print(test_encoced)\n",
    "            Id_to_tokens = tokenizer.convert_ids_to_tokens(test_encoced[1:-1])\n",
    "            print(Id_to_tokens)\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'word': '1937ë…„ 4ì›” 29ì¼', 'start_idx': 9, 'end_idx': 20, 'type': 'DAT'}\n",
      "[0, 20533, 2440, 24, 2429, 4346, 2210, 2]\n",
      "['1937', '##ë…„', '4', '##ì›”', '29', '##ì¼']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}