{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "import pandas as pd\n",
    "\n",
    "# pd.describe_option()\n",
    "#pandas column별 최대 출력 길이 조절\n",
    "pd.set_option(\"max_colwidth\",200)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.csv 불러오기, 중복 및 오라벨링 데이터 정리\n",
    "\n",
    "data = pd.read_csv(\"/opt/ml/dataset/train/train.csv\")\n",
    "# 완전 중복 제거 42개\n",
    "data = data.drop_duplicates(['sentence', 'subject_entity', 'object_entity', 'label'], keep='first')\n",
    "# 라벨링이 다른 데이터 제거\n",
    "data = data.drop(index=[6749, 8364, 22258, 277, 25094])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "data['subject_entity'] = data.subject_entity.map(eval)\n",
    "data['object_entity'] = data.object_entity.map(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no_relation', 'org:top_members/employees', 'per:employee_of', 'per:title', 'org:member_of', 'org:alternate_names', 'per:origin', 'org:place_of_headquarters', 'per:date_of_birth', 'per:alternate_names', 'per:spouse', 'per:colleagues', 'per:parents', 'org:founded', 'org:members', 'per:date_of_death', 'org:product', 'per:children', 'per:place_of_residence', 'per:other_family', 'per:place_of_birth', 'org:founded_by', 'per:product', 'per:siblings', 'org:political/religious_affiliation', 'per:religion', 'per:schools_attended', 'org:dissolved', 'org:number_of_employees/members', 'per:place_of_death']\n"
     ]
    }
   ],
   "source": [
    "#라벨 종류\n",
    "label_list = list(data.label.value_counts().index)\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#subject와 object type 정리\n",
    "def sbj_obj_info(row):\n",
    "    return row[\"subject_entity\"][\"type\"] + \"_\" + row[\"object_entity\"][\"type\"]\n",
    "\n",
    "def sbj_obj_len(row):\n",
    "    return row[\"subject_entity\"][\"word\"] + \"_\" + row[\"object_entity\"][\"word\"]\n",
    "\n",
    "data[\"sbj_obj_type\"] = data.apply(sbj_obj_info,axis=1)\n",
    "data[\"sbj_obj_word\"] = data.apply(sbj_obj_len,axis=1)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>sbj_obj_type</th>\n",
       "      <th>sbj_obj_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey Road》에 담은 노래다.</td>\n",
       "      <td>{'word': '비틀즈', 'start_idx': 24, 'end_idx': 26, 'type': 'ORG'}</td>\n",
       "      <td>{'word': '조지 해리슨', 'start_idx': 13, 'end_idx': 18, 'type': 'PER'}</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>ORG_PER</td>\n",
       "      <td>비틀즈_조지 해리슨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으로 재탄생한다.</td>\n",
       "      <td>{'word': '민주평화당', 'start_idx': 19, 'end_idx': 23, 'type': 'ORG'}</td>\n",
       "      <td>{'word': '대안신당', 'start_idx': 14, 'end_idx': 17, 'type': 'ORG'}</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikitree</td>\n",
       "      <td>ORG_ORG</td>\n",
       "      <td>민주평화당_대안신당</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                     sentence  \\\n",
       "0   0  〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey Road》에 담은 노래다.   \n",
       "1   1       호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으로 재탄생한다.   \n",
       "\n",
       "                                                     subject_entity  \\\n",
       "0    {'word': '비틀즈', 'start_idx': 24, 'end_idx': 26, 'type': 'ORG'}   \n",
       "1  {'word': '민주평화당', 'start_idx': 19, 'end_idx': 23, 'type': 'ORG'}   \n",
       "\n",
       "                                                       object_entity  \\\n",
       "0  {'word': '조지 해리슨', 'start_idx': 13, 'end_idx': 18, 'type': 'PER'}   \n",
       "1    {'word': '대안신당', 'start_idx': 14, 'end_idx': 17, 'type': 'ORG'}   \n",
       "\n",
       "         label     source sbj_obj_type sbj_obj_word  \n",
       "0  no_relation  wikipedia      ORG_PER   비틀즈_조지 해리슨  \n",
       "1  no_relation   wikitree      ORG_ORG   민주평화당_대안신당  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 label_sbj_obj_type 140개 중 1개의 조건에서 데이터가 1개씩만 존재\n"
     ]
    }
   ],
   "source": [
    "#라벨별 subject_object 빈도수 확인\n",
    "tmp = data.groupby([\"label\",\"sbj_obj_type\"]).sbj_obj_type.count().to_frame()\n",
    "tmp.columns = [\"freq\"]\n",
    "tmp = tmp.reset_index()\n",
    "\n",
    "#label별 type-freq\n",
    "c = 1\n",
    "from collections import defaultdict\n",
    "label_type_freq = defaultdict(dict)\n",
    "for i in range(len(tmp)):\n",
    "    row = tmp.iloc[i]\n",
    "    label,sbj_obj_type, freq = row['label'],row['sbj_obj_type'], row[\"freq\"]\n",
    "    label_type_freq[label][sbj_obj_type] = freq\n",
    "    # if freq ==1:\n",
    "        # print(f\"[{c}]{label}_{sbj_obj_type} : {freq}\")\n",
    "        # c+=1\n",
    "print(f\"전체 label_sbj_obj_type {len(tmp)}개 중 {c}개의 조건에서 데이터가 1개씩만 존재\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_relation : {'ORG_DAT': 1582, 'ORG_LOC': 548, 'ORG_NOH': 192, 'ORG_ORG': 1954, 'ORG_PER': 396, 'ORG_POH': 723, 'PER_DAT': 505, 'PER_LOC': 309, 'PER_NOH': 118, 'PER_ORG': 739, 'PER_PER': 1396, 'PER_POH': 1051}\n",
      "org:alternate_names : {'ORG_DAT': 1, 'ORG_LOC': 23, 'ORG_NOH': 5, 'ORG_ORG': 1153, 'ORG_PER': 31, 'ORG_POH': 105}\n",
      "org:dissolved : {'ORG_DAT': 66}\n",
      "org:founded : {'ORG_DAT': 450}\n",
      "org:founded_by : {'ORG_ORG': 5, 'ORG_PER': 144, 'ORG_POH': 5, 'PER_LOC': 1}\n",
      "org:member_of : {'ORG_DAT': 5, 'ORG_LOC': 173, 'ORG_NOH': 9, 'ORG_ORG': 1318, 'ORG_PER': 1, 'ORG_POH': 358}\n",
      "org:members : {'ORG_DAT': 1, 'ORG_LOC': 97, 'ORG_NOH': 2, 'ORG_ORG': 285, 'ORG_PER': 2, 'ORG_POH': 32, 'PER_PER': 1}\n",
      "org:number_of_employees/members : {'ORG_NOH': 48}\n",
      "org:place_of_headquarters : {'ORG_DAT': 4, 'ORG_LOC': 893, 'ORG_NOH': 2, 'ORG_ORG': 254, 'ORG_PER': 1, 'ORG_POH': 39, 'PER_LOC': 1}\n",
      "org:political/religious_affiliation : {'ORG_DAT': 1, 'ORG_LOC': 4, 'ORG_ORG': 54, 'ORG_PER': 1, 'ORG_POH': 38}\n",
      "org:product : {'ORG_LOC': 24, 'ORG_NOH': 1, 'ORG_ORG': 48, 'ORG_PER': 3, 'ORG_POH': 304}\n",
      "org:top_members/employees : {'ORG_LOC': 13, 'ORG_NOH': 1, 'ORG_ORG': 22, 'ORG_PER': 4190, 'ORG_POH': 52, 'PER_PER': 1}\n",
      "per:alternate_names : {'PER_LOC': 9, 'PER_NOH': 1, 'PER_ORG': 39, 'PER_PER': 879, 'PER_POH': 70}\n",
      "per:children : {'PER_DAT': 1, 'PER_LOC': 3, 'PER_NOH': 2, 'PER_ORG': 1, 'PER_PER': 275, 'PER_POH': 22}\n",
      "per:colleagues : {'PER_DAT': 1, 'PER_LOC': 1, 'PER_ORG': 10, 'PER_PER': 512, 'PER_POH': 10}\n",
      "per:date_of_birth : {'PER_DAT': 1128, 'PER_NOH': 2}\n",
      "per:date_of_death : {'PER_DAT': 409, 'PER_LOC': 2, 'PER_NOH': 2, 'PER_ORG': 1, 'PER_PER': 3}\n",
      "per:employee_of : {'PER_DAT': 19, 'PER_LOC': 165, 'PER_NOH': 5, 'PER_ORG': 2853, 'PER_PER': 391, 'PER_POH': 134}\n",
      "per:origin : {'PER_DAT': 60, 'PER_LOC': 821, 'PER_NOH': 2, 'PER_ORG': 267, 'PER_PER': 29, 'PER_POH': 55}\n",
      "per:other_family : {'PER_LOC': 4, 'PER_ORG': 1, 'PER_PER': 178, 'PER_POH': 7}\n",
      "per:parents : {'PER_DAT': 5, 'PER_LOC': 7, 'PER_NOH': 1, 'PER_PER': 441, 'PER_POH': 64}\n",
      "per:place_of_birth : {'PER_DAT': 1, 'PER_LOC': 161, 'PER_ORG': 3, 'PER_PER': 1}\n",
      "per:place_of_death : {'PER_DAT': 1, 'PER_LOC': 35, 'PER_ORG': 1, 'PER_PER': 1, 'PER_POH': 2}\n",
      "per:place_of_residence : {'PER_DAT': 4, 'PER_LOC': 172, 'PER_ORG': 11, 'PER_POH': 6}\n",
      "per:product : {'PER_LOC': 2, 'PER_ORG': 11, 'PER_PER': 6, 'PER_POH': 120}\n",
      "per:religion : {'PER_LOC': 2, 'PER_ORG': 80, 'PER_POH': 14}\n",
      "per:schools_attended : {'PER_LOC': 2, 'PER_ORG': 80}\n",
      "per:siblings : {'PER_PER': 113, 'PER_POH': 23}\n",
      "per:spouse : {'PER_DAT': 1, 'PER_LOC': 6, 'PER_ORG': 1, 'PER_PER': 761, 'PER_POH': 26}\n",
      "per:title : {'PER_DAT': 3, 'PER_LOC': 81, 'PER_NOH': 19, 'PER_ORG': 140, 'PER_PER': 14, 'PER_POH': 1842}\n"
     ]
    }
   ],
   "source": [
    "for key in label_type_freq.keys():\n",
    "    print(f\"{key} : {label_type_freq[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label - sub_obj - dictionary 만들기\n",
    "from collections import defaultdict\n",
    "sbj_obj_dict = defaultdict(dict)\n",
    "\n",
    "for label in label_list:\n",
    "    sbj_obj_type_list = data[data.label==label].sbj_obj_type.unique()\n",
    "    for sbj_obj_type in sbj_obj_type_list:\n",
    "        sbj_obj_dict[label][sbj_obj_type] = {}\n",
    "        sbj_obj_dict[label][sbj_obj_type][\"sbjs\"] = []\n",
    "        sbj_obj_dict[label][sbj_obj_type][\"objs\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32423/32423 [00:03<00:00, 9283.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(data))):\n",
    "    tmp_row = data.iloc[i]\n",
    "    label, sbj_obj_type = tmp_row[\"label\"],tmp_row[\"sbj_obj_type\"]\n",
    "    sbj,obj = tmp_row[\"sbj_obj_word\"].split(\"_\")\n",
    "    \n",
    "    sbj_obj_dict[label][sbj_obj_type][\"sbjs\"].append(sbj)\n",
    "    sbj_obj_dict[label][sbj_obj_type][\"objs\"].append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = \"per:title\"\n",
    "sbj_obj_type = \"PER_NOH\"\n",
    "len(sbj_obj_dict[label][sbj_obj_type][\"sbjs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbj_list = sbj_obj_dict[label][sbj_obj_type][\"sbjs\"]\n",
    "sbj_list_or = sbj_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['홍영표',\n",
       " '함태영',\n",
       " '함태영',\n",
       " '이학재',\n",
       " '김홍일',\n",
       " '부좌현',\n",
       " '유진산',\n",
       " '변인선',\n",
       " '홍영표',\n",
       " '김서형',\n",
       " '김홍일',\n",
       " '김대중',\n",
       " '정세균',\n",
       " '이석기',\n",
       " '이학재',\n",
       " '송석찬',\n",
       " '이낙연',\n",
       " '이석기',\n",
       " '유진산']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(2)\n",
    "[random.choice(sbj_list_or) for _ in range(len(sbj_list_or))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31852/364709847.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msbj_list\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0msbj_list_or\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msbj_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msbj_list_or\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msbj_list_or\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# sbj_list_or = sbj_list.copy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/random.py\u001b[0m in \u001b[0;36mseed\u001b[0;34m(self, a, version)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'big'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgauss_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while i<1000 or sbj_list!=sbj_list_or:\n",
    "    random.seed(2)\n",
    "    sbj_list = [random.choice(sbj_list_or) for _ in range(len(sbj_list_or))]\n",
    "# sbj_list_or = sbj_list.copy()\n",
    "    # random.shuffle(sbj_list)\n",
    "\n",
    "    if sbj_list==sbj_list_or:\n",
    "        print(i)\n",
    "        break\n",
    "    i+=1\n",
    "print(i)\n",
    "# sbj_list==sbj_list_or\n",
    "\n",
    "# [random.choice(sbj_list) for _ in range(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"per:title\"\n",
    "sbj_obj_type = \"PER_NOH\"\n",
    "sbj_list = sbj_obj_dict[label][sbj_obj_type][\"sbjs\"]\n",
    "obj_list = sbj_obj_dict[label][sbj_obj_type][\"objs\"]\n",
    "# resample=True\n",
    "# while resample==True:\n",
    "    # subject와 object entity 순서 shuffle\n",
    "    # random.seed(2)\n",
    "    # random.shuffle(sbj_list)\n",
    "    # random.shuffle(obj_list)\n",
    "\n",
    "    # #shuffle된 entity를 조합\n",
    "    # new_sb_ob_list = []\n",
    "    # for sb,ob in zip(sbj_list,obj_list):\n",
    "    #     new_sb_ob_list.append(f\"{sb}_{ob}\")\n",
    "    \n",
    "    # #기존 entity 조합과 비교\n",
    "    # data_cond = (data.label==label) & (data.sbj_obj_type==sbj_obj_type)\n",
    "    \n",
    "    # if not any(data[data_cond].sbj_obj_word==new_sb_ob_list): #전체가 false(모든 element 불일치)일 경우 return\n",
    "    #     resample=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = data[data_cond]\n",
    "target_data[\"replaced\"] = new_sb_ob_list\n",
    "target_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31852/1759374625.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# id    sentence        subject_entity  object_entity   label   source  sbj_obj_type    sbj_obj_word    rget_data.head(2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# target_data.iloc[:1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mttt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_entity_loc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;31m# target_data.iloc[:2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_data' is not defined"
     ]
    }
   ],
   "source": [
    "def check_entity_loc(row):\n",
    "    row = row.copy()\n",
    "    sent = row['sentence']\n",
    "    sbjs, objs = row['subject_entity'], row['object_entity']\n",
    "    new_sb, new_ob = row['replaced'].split(\"_\")\n",
    "\n",
    "    sb_len, new_sb_len, ob_len, new_ob_lens = len(sbjs['word']), len(new_sb), len(objs['word']), len(new_ob)\n",
    "    if sbjs['start_idx'] < objs['start_idx'] : \n",
    "        str1,str2,str3 = sent[:sbjs['start_idx']], sent[sbjs['end_idx']+1:objs['start_idx']], sent[objs['end_idx']+1:]\n",
    "        new_sent = str1 + new_sb + str2 + new_ob + str3\n",
    "        sbjs['word'] = new_sb\n",
    "        sbjs[\"end_idx\"] = sbjs['start_idx'] + len(new_sb)-1\n",
    "        objs['word'] = new_ob\n",
    "        objs['start_idx'] = new_sent.find(new_ob)\n",
    "        objs['end_idx'] = objs['start_idx'] + len(new_ob)-1\n",
    "    else:\n",
    "        str1,str2,str3 = sent[:objs['start_idx']], sent[objs['end_idx']+1:sbjs['start_idx']], sent[sbjs['end_idx']+1:]\n",
    "        new_sent = str1 + new_ob + str2 + new_sb + str3\n",
    "        objs['word'] = new_ob\n",
    "        objs[\"end_idx\"] = objs['start_idx'] + len(new_sb)-1\n",
    "        sbjs['word'] = new_sb\n",
    "        sbjs['start_idx'] = new_sent.find(new_sb)\n",
    "        sbjs['end_idx'] = sbjs['start_idx'] + len(new_sb)-1\n",
    "    \n",
    "    return {\"id\" : row['id'],\n",
    "            \"sentence\" : new_sent,\n",
    "            \"subject_entity\" : sbjs,\n",
    "            \"object_entity\" : objs,\n",
    "            \"label\" : row[\"label\"],\n",
    "            \"source\" : row[\"source\"],\n",
    "            \"sbj_obj_type\" : row[\"sbj_obj_type\"],\n",
    "            \"sbj_obj_word\" : row[\"sbj_obj_word\"]}\n",
    "\n",
    "# id\tsentence\tsubject_entity\tobject_entity\tlabel\tsource\tsbj_obj_type\tsbj_obj_word\trget_data.head(2)\n",
    "# target_data.iloc[:1]\n",
    "ttt = target_data[:2].apply(check_entity_loc,axis=1)\n",
    "# target_data.iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(target_data.iloc[:2].apply(check_entity_loc,axis=1).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = pd.DataFrame(list(target_data.apply(check_entity_loc,axis=1).values))\n",
    "ttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat(ttt,data[:2])\n",
    "pd.concat([data,pd.DataFrame(list(target_data.iloc[:2].apply(check_entity_loc,axis=1).values))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd_dataset = pd.read_csv('/opt/ml/dataset/train/train.csv')\n",
    "    \n",
    "# 완전 중복 제거 42개\n",
    "pd_dataset = pd_dataset.drop_duplicates(['sentence', 'subject_entity', 'object_entity', 'label'], keep='first')\n",
    "# 라벨링이 다른 데이터 제거\n",
    "pd_dataset = pd_dataset.drop(index=[6749, 8364, 22258, 277, 25094])\n",
    "pd_dataset = pd_dataset.reset_index(drop=True)\n",
    "print(\"Finish remove duplicated data\")\n",
    "#datatype 변경\n",
    "pd_dataset['subject_entity'] = pd_dataset.subject_entity.map(eval)\n",
    "pd_dataset['object_entity'] = pd_dataset.object_entity.map(eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir, entity_flag=0, preprocessing_flag=None, mecab_flag=0,seed=2):\n",
    "    \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\n",
    "    pd_dataset = pd.read_csv(dataset_dir)\n",
    "    \n",
    "    \n",
    "    # 완전 중복 제거 42개\n",
    "    pd_dataset = pd_dataset.drop_duplicates(['sentence', 'subject_entity', 'object_entity', 'label'], keep='first')\n",
    "    # 라벨링이 다른 데이터 제거\n",
    "    pd_dataset = pd_dataset.drop(index=[6749, 8364, 22258, 277, 25094])\n",
    "    pd_dataset = pd_dataset.reset_index(drop=True)\n",
    "    # pd_dataset = pd_dataset.iloc\n",
    "    print(\"Finish remove duplicated data\")\n",
    "\n",
    "    #datatype 변경\n",
    "    pd_dataset['subject_entity'] = pd_dataset.subject_entity.map(eval)\n",
    "    pd_dataset['object_entity'] = pd_dataset.object_entity.map(eval)\n",
    "    \n",
    "    #data augmentation(동일한 label 및 subject_object 내에서 subject와 object entity를 sampling하여 데이터 증량)\n",
    "    #단 특정 subject_object_type은 data가 1개이므로, 이 경우 임의로 값 지정\n",
    "    pd_dataset = augmentation_by_resampling(pd_dataset,seed)\n",
    "\n",
    "    return pd_dataset\n",
    "\n",
    "\n",
    "def augmentation_by_resampling(data,seed):\n",
    "    #subject_object 타입 및 word를 새로운 column에 추가(마지막에 삭제 예정)\n",
    "    def sbj_obj_type(row):\n",
    "        return row[\"subject_entity\"][\"type\"] + \"_\" + row[\"object_entity\"][\"type\"]\n",
    "    def sbj_obj_word(row):\n",
    "        return row[\"subject_entity\"][\"word\"] + \"_\" + row[\"object_entity\"][\"word\"]\n",
    "\n",
    "    data[\"sbj_obj_entity_type\"] = data.apply(sbj_obj_type,axis=1)\n",
    "    data[\"sbj_obj_entity_word\"] = data.apply(sbj_obj_word,axis=1)\n",
    "       \n",
    "\n",
    "    #라벨 종류 확인\n",
    "    label_list = get_labels(data)\n",
    "\n",
    "    #label_sbj_obj - dict 생성\n",
    "    sbj_obj_dict = get_dict_frame(data,label_list)\n",
    "\n",
    "    #dictionary에 값 입력 - label - sbj_obj_entity_type - sbj / obj\n",
    "    print(\"데이터 수집 시작\")\n",
    "    for i in range(len(data)):\n",
    "        tmp_row = data.iloc[i]\n",
    "        label, sbj_obj_entity_type = tmp_row[\"label\"],tmp_row[\"sbj_obj_entity_type\"]\n",
    "        sbj,obj = tmp_row[\"sbj_obj_entity_word\"].split(\"_\")\n",
    "        sbj_obj_dict[label][sbj_obj_entity_type][\"sbjs\"].append(sbj)\n",
    "        sbj_obj_dict[label][sbj_obj_entity_type][\"objs\"].append(obj)\n",
    "    \n",
    "    print(\"데이터 샘플링 시작\")\n",
    "    #sampling\n",
    "    print(\"샘플링 시작 시: \", len(data))\n",
    "    for label in label_list:\n",
    "        for sbj_obj_entity_type in sbj_obj_dict[label].keys():\n",
    "            print(label,\"_\",sbj_obj_entity_type)\n",
    "            if len(sbj_obj_dict[label][sbj_obj_entity_type]['sbjs'])>5: #data개수가 1개이면 shuffle이 불가하므로 pass\n",
    "                new_sbj_obj = shuffling_data(data,sbj_obj_dict,label,sbj_obj_entity_type, seed)\n",
    "                data = augmentation(data,new_sbj_obj,label,sbj_obj_entity_type)\n",
    "                print(\"샘플링 중\", len(data))\n",
    "    # data = data.drop(['sbj_obj_entity_type','sbj_obj_entity_word'])\n",
    "    return data\n",
    "\n",
    "             \n",
    "\n",
    "def get_labels(data):\n",
    "    return list(data.label.value_counts().index)\n",
    "\n",
    "\n",
    "def get_dict_frame(data,label_list):\n",
    "    sbj_obj_dict = collections.defaultdict(dict)\n",
    "    for label in label_list:\n",
    "        sbj_obj_type_list = data.loc[data.label==label,\"sbj_obj_entity_type\"].unique()\n",
    "        for sbj_obj_type in sbj_obj_type_list:\n",
    "            sbj_obj_dict[label][sbj_obj_type] = {}\n",
    "            sbj_obj_dict[label][sbj_obj_type][\"sbjs\"] = []\n",
    "            sbj_obj_dict[label][sbj_obj_type][\"objs\"] = []\n",
    "    return sbj_obj_dict\n",
    "\n",
    "\n",
    "def shuffling_data(data,sbj_obj_dict,label,sbj_obj_type, seed=2):\n",
    "    sbj_list = sbj_obj_dict[label][sbj_obj_type][\"sbjs\"]\n",
    "    obj_list = sbj_obj_dict[label][sbj_obj_type][\"objs\"]\n",
    "    resample=True\n",
    "    while resample==True:\n",
    "        #subject와 object entity 순서 shuffle\n",
    "        random.seed(seed)\n",
    "        random.shuffle(sbj_list)\n",
    "        random.shuffle(obj_list)\n",
    "        sbj_list_choice = [random.choice(sbj_list) for _ in range(len(sbj_list))]\n",
    "        obj_list_choice = [random.choice(obj_list) for _ in range(len(obj_list))]\n",
    "\n",
    "        #shuffle된 entity를 조합\n",
    "        new_sb_ob_list = []\n",
    "        for sb,ob in zip(sbj_list_choice,obj_list_choice):\n",
    "            new_sb_ob_list.append(f\"{sb}_{ob}\")\n",
    "        \n",
    "        #기존 entity 조합과 비교\n",
    "        data_cond = (data[\"label\"]==label) & (data[\"sbj_obj_entity_type\"]==sbj_obj_type)\n",
    "        \n",
    "        if not any(data.loc[data_cond,\"sbj_obj_entity_word\"]==new_sb_ob_list): #전체가 false(모든 element 불일치)일 경우 return\n",
    "            return new_sb_ob_list\n",
    "\n",
    "def augmentation(data,new_sb_ob_list, label,sbj_obj_type):\n",
    "    data_cond = (data.label==label) & (data.sbj_obj_entity_type==sbj_obj_type)\n",
    "    target_data = data[data_cond].copy()\n",
    "    target_data['replaced'] = new_sb_ob_list\n",
    "    new_data = pd.DataFrame(list(target_data.apply(change_entity,axis=1).values))\n",
    "    data = pd.concat([data, new_data])\n",
    "    return data\n",
    "\n",
    "def change_entity(row):\n",
    "    row = row.copy()\n",
    "    sent = row['sentence']\n",
    "    sbjs, objs = row['subject_entity'], row['object_entity']\n",
    "    new_sb, new_ob = row['replaced'].split(\"_\")\n",
    "\n",
    "    sb_len, new_sb_len, ob_len, new_ob_lens = len(sbjs['word']), len(new_sb), len(objs['word']), len(new_ob)\n",
    "    if sbjs['start_idx'] < objs['start_idx'] : \n",
    "        str1,str2,str3 = sent[:sbjs['start_idx']], sent[sbjs['end_idx']+1:objs['start_idx']], sent[objs['end_idx']+1:]\n",
    "        new_sent = str1 + new_sb + str2 + new_ob + str3\n",
    "        sbjs['word'] = new_sb\n",
    "        sbjs[\"end_idx\"] = sbjs['start_idx'] + len(new_sb)-1\n",
    "        objs['word'] = new_ob\n",
    "        objs['start_idx'] = new_sent.find(new_ob)\n",
    "        objs['end_idx'] = objs['start_idx'] + len(new_ob)-1\n",
    "    else:\n",
    "        str1,str2,str3 = sent[:objs['start_idx']], sent[objs['end_idx']+1:sbjs['start_idx']], sent[sbjs['end_idx']+1:]\n",
    "        new_sent = str1 + new_ob + str2 + new_sb + str3\n",
    "        objs['word'] = new_ob\n",
    "        objs[\"end_idx\"] = objs['start_idx'] + len(new_sb)-1\n",
    "        sbjs['word'] = new_sb\n",
    "        sbjs['start_idx'] = new_sent.find(new_sb)\n",
    "        sbjs['end_idx'] = sbjs['start_idx'] + len(new_sb)-1\n",
    "    \n",
    "    return {\"id\" : row['id'],\n",
    "            \"sentence\" : new_sent,\n",
    "            \"subject_entity\" : sbjs,\n",
    "            \"object_entity\" : objs,\n",
    "            \"label\" : row[\"label\"],\n",
    "            \"source\" : row[\"source\"],\n",
    "            \"sbj_obj_entity_type\" : row[\"sbj_obj_entity_type\"],\n",
    "            \"sbj_obj_entity_word\" : row[\"sbj_obj_entity_type\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish remove duplicated data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "dataset_dir = '/opt/ml/dataset/train/train.csv'\n",
    "pd_dataset = pd.read_csv(dataset_dir)\n",
    "    \n",
    "if 'train' in dataset_dir:\n",
    "    # 완전 중복 제거 42개\n",
    "    pd_dataset = pd_dataset.drop_duplicates(['sentence', 'subject_entity', 'object_entity', 'label'], keep='first')\n",
    "    # 라벨링이 다른 데이터 제거\n",
    "    pd_dataset = pd_dataset.drop(index=[6749, 8364, 22258, 277, 25094])\n",
    "    pd_dataset = pd_dataset.reset_index(drop=True)\n",
    "    print(\"Finish remove duplicated data\")\n",
    "\n",
    "#datatype 변경\n",
    "pd_dataset['subject_entity'] = pd_dataset.subject_entity.map(eval)\n",
    "pd_dataset['object_entity'] = pd_dataset.object_entity.map(eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd_dataset[[\"id\"]]#,\"sentence\",'subject_entity',\"object_entity\",\"source\"]]\n",
    "y = pd_dataset[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      no_relation\n",
       "1                      no_relation\n",
       "2                    org:member_of\n",
       "3        org:top_members/employees\n",
       "4                      no_relation\n",
       "                   ...            \n",
       "32418              per:employee_of\n",
       "32419               per:colleagues\n",
       "32420    org:top_members/employees\n",
       "32421                  no_relation\n",
       "32422    org:top_members/employees\n",
       "Name: label, Length: 32423, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True,random_state=2)\n",
    "Xs = []\n",
    "ys = []\n",
    "for i,(train_idx,valid_idx) in enumerate(skf.split(X,y)):\n",
    "    train_dataset, valid_dataset = pd_dataset.iloc[train_idx], pd_dataset.iloc[valid_idx]\n",
    "    break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
